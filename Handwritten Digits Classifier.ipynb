{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digits Classifier\n",
    "Dataset: MNIST\n",
    "\n",
    "Model: LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import torch \n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data\n",
    "In the MNIST dataset, each image has 1 channel (grayscale), is 28*28 pixels, and is accompanied with its target label.\n",
    "\n",
    "The train set has 60,000 images and the test set has 10,000 images.\n",
    "\n",
    "For other specifics, visit the official website: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "You don't have to worry about navigating this page directly, as PyTorch has pre-loaded datasets.\n",
    "\n",
    "During the preprocessing, we resize to 32*32 px since the LeNet-5 architecture takes in images of those dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "transform = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), transforms.Normalize((0.5), (0.5))]) \n",
    "#load train dataset:\n",
    "trainset = torchvision.datasets.MNIST(root='~/.pytorch/MNIST_data/', train = True, download = True, transform = transform)  \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle = True, num_workers = 2, drop_last = True)\n",
    "#load test dataset:\n",
    "testset = torchvision.datasets.MNIST(root = '~/.pytorch/MNIST_data/', train = False, download = True, transform = transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle = False, num_workers=2, drop_last = True)\n",
    "\n",
    "classes = (\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\") #Just labels, for our own printing use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A class to encapsulate the network architecture\n",
    "class Net(nn.Module): \n",
    "  def __init__(self): \n",
    "    super(Net, self).__init__() \n",
    "    self.conv1 = nn.Conv2d(1,6,5) \n",
    "    self.pool = nn.MaxPool2d(2,2) \n",
    "    self.conv2 = nn.Conv2d(6,16,5) \n",
    "    self.fc1 = nn.Linear(16*5*5,120) \n",
    "    self.fc2 = nn.Linear(120,84)\n",
    "    self.fc3 = nn.Linear(84,10)\n",
    "\n",
    "  def forward(self, x): \n",
    "    x = self.pool(F.relu(self.conv1(x))) \n",
    "    x = self.pool(F.relu(self.conv2(x)))\n",
    "    x = x.view(-1, 16*5*5) \n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.fc3(x) #The softmax activation isn't done here as combining it with loss calculations will make it more computationally efficient. Hence, this will automatically be taken care of by the loss criterion that we chose later\n",
    "    return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "#Loss function and Optimizer:\n",
    "criterion = nn.CrossEntropyLoss() # This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "The inner loop runs 15k times, where the loss is reset every 2000th batch. This leaves space for 1000 batches' loss to be recorded, as statistics for every epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_minibatches=[] #x and y for plotting the training curve later\n",
    "y_loss=[]\n",
    "optimizer.zero_grad()\n",
    "running_loss = 0.0\n",
    "for epoch in range(3): \n",
    "  print(\"epoch loss:\", running_loss/1000)\n",
    "  running_loss = 0.0\n",
    "  for i, data in enumerate(trainloader, 0): #i is also the no. of minibatches\n",
    "     inputs, labels = data\n",
    "     optimizer.zero_grad() \n",
    "     outputs = net(inputs) #Forward prop\n",
    "     loss = criterion(outputs, labels) \n",
    "     loss.backward() #Backward prop\n",
    "     optimizer.step()  #Optimizer finally updates the parameters\n",
    "    \n",
    "     #Record some statistics:\n",
    "     running_loss += loss.item() \n",
    "     if i%2000 == 1999: #For every 2000 minibatches (or 8k images)\n",
    "       print('[%d, %d] loss: %.3f' %(epoch+1, i+1, running_loss/2000)) \n",
    "       x_minibatches.append((i+1)*4+(epoch*60000))\n",
    "       y_loss.append(running_loss/2000)\n",
    "       running_loss = 0.0 #Reset the loss, so we know how the loss changes within a reasonable frame\n",
    "print('Finished training')\n",
    "\n",
    "#Demo: We save, and later load, our model:\n",
    "PATH = './MNIST_net.pth' #By convention, PyTorch files have pt/pth extension\n",
    "torch.save(net.state_dict(), PATH) #recommended to save state dict rather than module for compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peeking into the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normal plot\n",
    "plt.plot(x_minibatches, y_loss, marker='.', markerfacecolor='g') \n",
    "plt.xlabel(\"Dataset images progressed through\")\n",
    "plt.ylabel(\"Avg. Loss\")\n",
    "plt.title('Loss per 8000 images (loss reset every 8000 imgs)')\n",
    "plt.show()\n",
    "\n",
    "#Zoom in, as the first point is the farthest away from others, making it impossible to see the actual progress in later epochs\n",
    "plt.ylim(top=0.225)\n",
    "plt.plot(x_minibatches, y_loss, marker='.', markerfacecolor='g') \n",
    "plt.xlabel(\"Dataset images progressed through\")\n",
    "plt.ylabel(\"Avg. Loss\")\n",
    "plt.title('Loss per 8000 images (loss reset every 8000 imgs)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A live sample test of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img): #A function to plot image(s)\n",
    "  #we first undo some of the preprocessing to see how the actual image is\n",
    "  img = img/2 + 0.5 #un-normalize \n",
    "  npimg = img.numpy() #convert from Tensor back to numpy's ndarray (needed for the transpose() below)\n",
    "  plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "  plt.show() \n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH)) \n",
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs, 1) \n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model\n",
    "We test on the test set first, then the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test set\n",
    "correct = 0.0\n",
    "total = 0.0\n",
    "with torch.no_grad(): \n",
    "  for data in testloader: \n",
    "    images, labels = data\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1) \n",
    "    total+=labels.size(0) \n",
    "    correct += (predicted==labels).sum().item() \n",
    "print('Test set accuracy: %f %%' %(100.0*correct/total))\n",
    "\n",
    "#Train set\n",
    "correct = 0.0\n",
    "total = 0.0\n",
    "with torch.no_grad():\n",
    "  for data in trainloader:\n",
    "    images, labels = data\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total+=labels.size(0) \n",
    "    correct += (predicted==labels).sum().item() \n",
    "print('Train set accuracy: %f %%' %(100.0*correct/total))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
